<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>VSP</title>
    <link
      href="//fonts.googleapis.com/css?family=Nunito:400,700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="assets/css/style-starter.css" />
  </head>

  <body>
    <header>
      <div class="w3l-header" id="home">
        <div class="container">
          <nav class="navbar navbar-expand-lg navbar-dark pl-0 pr-0">
            <a class="navbar-brand m-0" href="index.html"
              ><span class="fa fa-gamepad"></span> VSP
            </a>
            <span class="navbar-toggler-icon fa fa-bars"></span>
            <div class="collapse navbar-collapse" id="navbarNav">
              <ul class="navbar-nav ml-auto">
                <li class="nav-item mr-lg-4">
                  <a class="nav-link pl-0 pr-0" href="../index.html">Home</a>
                  <a class="nav-link pl-0 pr-0" href="../blogs.html">Blogs</a>
                </li>
              </ul>
            </div>
          </nav>
        </div>
      </div>
    </header>

    <section class="w3l-about-bottom py-5" id="about">
      <div class="container py-lg-5 py-md-3">
        <div class="row middle-grids">
          <div class="col-lg-12 advantage-grid-info">
            <div class="advantage_left">
              <h1>Can India build a soverign model or do we need to?</h1>
              The GPU embargo forced China to innovate on
              <h3>Multi Head Latent Attention</h3>
              <h3>Multi Token Prediction</h3>
              <h3>Smarter Mixture of Experts</h3>
              <h3>Custom CUDA accleration</h3>
              <h3>FP8 low precision training</h3>
              <h3>GPRO</h3>
              Other innovations by GPU poors
              <h3>Kokoro-82M</h3>
              SOTA TTS model trained in $1000.
              <h3>Diffusion > $2000</h3>
              Using patch mixing and masking.
              <h3>Sky-T1</h3>
              Using data curation and distillation. <br />
              Data Generation - Rejection sampling from QwQ reasoning model
              <br />Training - Supervised FT a non reasoning model using 17k
              data <br />Evaluation - Math 500, AIME24, LiveCodeBench, GPQA
              <br /><br />
              Major AI players in India as of Feb 2025 are <br />* Krutrim by
              Bhavish Agarwal <br />* Sarvam AI <br />* Lossfunk by Paras Chopra
              (who recently sold his company Wingify for $200 million) <br />*
              Two Platforms by the GOAT Pranav Mistry (Silicon valley based
              company primarily working on Indian datasets) (Jio invested
              $15million for 25% stake)<br />* AI4Bharat (research lab in IIT
              Madras) <br /><br />
              <h1>TWO Platforms</h1>
              Pranav Mistry left Samsung in 2021 to start TWO Platforms. They
              introduced <br />* <b>SUTRA-V1</b> Multilingual language chat
              model. Dense architecture and 36b parameter.<br />*
              <b>SUTRA-R0</b> Reasoning model. Dense architecture and 8b & 36b
              parameters available. Beats o1 and R1 in MMLU in most Indian
              languages. <br />* <b>SUTRA-P0</b> (Coming soon). Time-series
              predicitive AI model - model which can forecast the future based
              on historical time-series data. <br />
              * <b>Dual transformer architecture</b> <br />
              <b>* MMLU benchmark</b> SUTRA has achieved 20-25% better
              performance in MMLU bechmark in Hindi, Korean, Gujarati, Japanese
              and Arabic as compared to GPT-4O. <br />
              <b>* Tokenization</b> The SUTRA models uses 4-8x less tokens for
              non-Romanized languages. They are claiming they outperformed all
              other LLMs in tokenization in 22 Indian official languages.

              <h2>Krutrim AI</h2>
              Models available on huggingface account of Krutrim as of febrauray
              2025 are Dhwani-1, Krutrim-2, Krutrim-translate, Chitrath-1,
              Vyakhyarth-1.
              <br />* Krutrim-1 (7B model) (Jan 2024) <br />* 1. Krutrim-2 (Feb
              2024) <br />* 2. Chitrath-1 (Vision language model build on top of
              K-1) <br />
              * 3. Dhwani-1 (ASR model build on top of K-1) <br />* 4.
              Vyakhyarth-1 (Embedding model for RAG & Search) <br />* 5. Krutrim
              Translate-1 (Translation model) <br />* BharatBench to evaluate
              Indic performaces <br />* Promised to build the largest
              supercomputer in India by the end of the year. <br />* Deployed
              deepseek-r1 on Krutrim cloud.

              <h2>Lossfunk</h2>
              * Exploring methods to uncensor Deepseek using abliteration <br />
              * Paras mentioned about using less data for training and more data
              for testing just like how human intelligence works

              <h2>Sarvam AI</h2>
              They have open sourced Shuka-v1 and Sarvam-2B on their huggingface
              platform. <br />
              <b>Sarvam-1</b>(October 2024) Indic 2b parameter model build on
              top of 10 Indic languages. Better performace on benchmarks MMLU,
              ARC-C, TriviaQA, BoolQ
              <br />
              <b>Shuka-V1</b> (Aug 2024) India's first OS audio language model
              which uses llama as decoder. <br />
              <b>Mayura</b> Translation <br />
              <b>Saraas</b> ASR with translation<br />
              <b>Bulbul</b> TTS <br />
              <b>Saarika</b> STT <br />

              <h2>AI4Bharat</h2>
              <b>IndicTrans-2</b>(May 2023) Machine translation on 22 Indian
              official languages. <b>Aksharantar</b>Largest publicly available
              transliteration dataset for Indian languages
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer>
      <section class="w3l-footers-1">
        <div class="footer py-3">
          <div class="container">
            <div class="footer-content">
              <div class="row">
                <div class="col-lg-8 footer-left">
                  <p class="m-0">
                    &copy; 2025 All Rights Reserved | Design by VSP
                  </p>
                </div>
                <div
                  class="col-lg-4 footer-right text-lg-right text-center mt-lg-0 mt-3"
                >
                  <ul class="social m-0 p-0">
                    <li>
                      <a href="https://github.com/vishnusureshperumbavoor"
                        >Github<span class="fa fa-github"></span
                      ></a>
                    </li>
                    <li>
                      <a
                        href="https://www.linkedin.com/in/vishnu-suresh-perumbavoor-9a7a8223a/"
                        >LinkedIn<span class="fa fa-linkedin"></span
                      ></a>
                    </li>
                    <li>
                      <a href="https://t.me/vishnusureshperumbavoor"
                        >Telegram<span class="fa fa-telegram"></span
                      ></a>
                    </li>
                    <li>
                      <a
                        href="https://www.instagram.com/vishnusureshperumbavoor/"
                        >Instagram<span class="fa fa-instagram"></span
                      ></a>
                    </li>
                    <li>
                      <a href="http://www.twitter.com/vspeeeeee"
                        >Twitter/X<span class="fa fa-twitter"></span
                      ></a>
                    </li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
    </footer>
  </body>
</html>
<style>
  body {
    background-color: #000;
    color: #7cfc00;
  }
  ul li a {
    color: #7df9ff;
  }
</style>
